{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation의 중요성\n",
    "\n",
    "서비스를 앉어적으로 운영하기 위해서\n",
    "사용자가 정확한 정보를 받을 수 있는지\n",
    "\n",
    "\"할루시네이션\" -> \"환각\"\n",
    "LLM이 잘못된 답변을 생성하는 경우\n",
    "\n",
    "## Dataset -> 도메인 전문가가 작성한 \"정답지\"\n",
    "특정 질문이 들어오면 -> 이런 답변을 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.121)\n",
      "Requirement already satisfied: openai in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.45.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain-pinecone in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith) (2.32.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith) (2.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-pinecone) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2.2.3)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (1.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langsmith) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ha018\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langsmith openai python-dotenv langchain langchain-openai langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"income_tax_dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input_question\": \"제1조에 따른 소득세법의 목적은 무엇인가요?\"},\n",
    "        {\"input_question\": \"'거주자'는 소득세법에서 어떻게 정의되나요?\"},\n",
    "        {\"input_question\": \"'비거주자'는 소득세법에 따라 어떻게 정의되나요?\"},\n",
    "        {\"input_question\": \"소득세법에 따른 '내국법인'은 누구를 의미하나요?\"},\n",
    "        {\"input_question\": \"소득세법에 따라 소득세를 납부할 의무가 있는 사람은 누구인가요?\"},\n",
    "        {\"input_question\": \"거주자의 과세 범위는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세법에 따라 소득은 어떻게 분류되나요?\"},\n",
    "        {\"input_question\": \"종합소득이란 무엇인가요?\"},\n",
    "        {\"input_question\": \"세금이 면제되는 소득의 종류는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세의 과세기간은 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"거주자의 소득세 납세지는 어디인가요?\"},\n",
    "        {\"input_question\": \"비거주자의 소득세 납세지는 어디인가요?\"},\n",
    "        {\"input_question\": \"납세지가 불분명한 경우 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"원천징수세액의 납세지는 어떻게 결정되나요?\"},\n",
    "        {\"input_question\": \"납세자의 사망 시 납세지는 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"신탁 소득에 대한 납세의 범위는 무엇인가요?\"},\n",
    "        {\"input_question\": \"원천징수 대상 소득은 무엇인가요?\"},\n",
    "        {\"input_question\": \"공동 소유 자산의 양도소득은 어떻게 과세되나요?\"},\n",
    "        {\"input_question\": \"이자 소득의 출처는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세법에서 배당소득은 어떻게 정의되나요?\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"output_answer\": \"소득세법의 목적은 소득의 성격과 납세자의 부담능력에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지하는 것입니다.\"},\n",
    "        {\"output_answer\": \"'거주자'는 한국에 주소를 두거나 183일 이상 거소를 둔 개인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"'비거주자'는 거주자가 아닌 개인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"'내국법인'은 법인세법 제2조 제1호에 따른 내국법인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"거주자 및 국내원천소득이 있는 비거주자는 소득세를 납부할 의무가 있습니다.\"},\n",
    "        {\"output_answer\": \"거주자는 법에서 규정한 모든 소득에 대해 과세되며, 비거주자는 국내원천소득에 대해서만 과세됩니다.\"},\n",
    "        {\"output_answer\": \"소득은 종합소득, 퇴직소득, 양도소득으로 분류됩니다.\"},\n",
    "        {\"output_answer\": \"종합소득은 이자소득, 배당소득, 사업소득, 근로소득, 연금소득 및 기타소득을 포함합니다.\"},\n",
    "        {\"output_answer\": \"비과세 소득에는 공익신탁의 이익, 특정 사업소득 및 기타 법에서 정한 특정 소득이 포함됩니다.\"},\n",
    "        {\"output_answer\": \"소득세의 과세기간은 매년 1월 1일부터 12월 31일까지입니다.\"},\n",
    "        {\"output_answer\": \"거주자의 소득세 납세지는 주소지이며, 주소지가 없으면 거소지입니다.\"},\n",
    "        {\"output_answer\": \"비거주자의 소득세 납세지는 국내사업장의 소재지입니다. 국내사업장이 여러 곳인 경우 주된 사업장의 소재지가 납세지가 됩니다.\"},\n",
    "        {\"output_answer\": \"납세지가 불분명한 경우 대통령령으로 정합니다.\"},\n",
    "        {\"output_answer\": \"원천징수세액의 납세지는 원천징수자의 종류와 위치에 따라 결정됩니다.\"},\n",
    "        {\"output_answer\": \"납세자의 사망 시 상속인 또는 납세관리인의 주소지나 거소지가 납세지가 됩니다.\"},\n",
    "        {\"output_answer\": \"신탁 소득에 대한 납세의 범위는 신탁의 수익자가 해당 소득에 대해 납세의무를 집니다.\"},\n",
    "        {\"output_answer\": \"이자소득, 배당소득 및 기타 법에서 정한 소득은 원천징수 대상입니다.\"},\n",
    "        {\"output_answer\": \"공동 소유 자산의 양도소득은 각 거주자 소유 지분에 따라 과세됩니다.\"},\n",
    "        {\"output_answer\": \"이자 소득의 출처는 정부 및 지방자치단체가 발행한 채권, 법인이 발행한 채권, 국내외 은행 예금 등입니다.\"},\n",
    "        {\"output_answer\": \"배당소득은 국내외 법인으로부터 받는 배당금 및 배분금, 기타 법에서 정한 소득을 포함합니다.\"}\n",
    "    ],\n",
    "    metadata= [\n",
    "        {\"contexts\": \"제1조(목적) 이 법은 개인의 소득에 대하여 소득의 성격과 납세자의 부담능력 등에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지함을 목적으로 한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “거주자”란 국내에 주소를 두거나 183일 이상의 거소를 둔 개인을 말한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “비거주자”란 거주자가 아닌 개인을 말한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “내국법인”이란 「법인세법」 제2조제1호에 따른 내국법인을 말한다.\"},\n",
    "        {\"contexts\": \"제2조(납세의무) 거주자 및 국내원천소득이 있는 비거주자는 소득세를 납부할 의무가 있다.\"},\n",
    "        {\"contexts\": \"제3조(과세소득의 범위) 거주자는 법에서 규정한 모든 소득에 대해 과세되며, 비거주자는 국내원천소득에 대해서만 과세된다.\"},\n",
    "        {\"contexts\": \"제4조(소득의 구분) 소득은 종합소득, 퇴직소득, 양도소득으로 분류된다.\"},\n",
    "        {\"contexts\": \"제4조(소득의 구분) 종합소득은 이자소득, 배당소득, 사업소득, 근로소득, 연금소득 및 기타소득을 포함한다.\"},\n",
    "        {\"contexts\": \"제12조(비과세소득) 비과세 소득에는 공익신탁의 이익, 특정 사업소득 및 기타 법에서 정한 특정 소득이 포함된다.\"},\n",
    "        {\"contexts\": \"제5조(과세기간) 소득세의 과세기간은 매년 1월 1일부터 12월 31일까지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 거주자의 소득세 납세지는 주소지이며, 주소지가 없으면 거소지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 비거주자의 소득세 납세지는 국내사업장의 소재지이다. 국내사업장이 여러 곳인 경우 주된 사업장의 소재지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 납세지가 불분명한 경우에는 대통령령으로 정한다.\"},\n",
    "        {\"contexts\": \"제7조(원천징수 등의 경우의 납세지) 원천징수세액의 납세지는 원천징수자의 종류와 위치에 따라 결정된다.\"},\n",
    "        {\"contexts\": \"제8조(상속 등의 경우의 납세지) 납세자의 사망 시 상속인 또는 납세관리인의 주소지나 거소지가 납세지가 된다.\"},\n",
    "        {\"contexts\": \"제2조의3(신탁재산 귀속 소득에 대한 납세의무의 범위) 신탁 소득에 대한 납세의 범위는 신탁의 수익자가 해당 소득에 대해 납세의무를 진다.\"},\n",
    "        {\"contexts\": \"제14조(과세표준의 계산) 이자소득, 배당소득 및 기타 법에서 정한 소득은 원천징수 대상이다.\"},\n",
    "        {\"contexts\": \"제14조(과세표준의 계산) 공동 소유 자산의 양도소득은 각 거주자 소유 지분에 따라 과세된다.\"},\n",
    "        {\"contexts\": \"제16조(이자소득) 이자 소득의 출처는 정부 및 지방자치단체가 발행한 채권, 법인이 발행한 채권, 국내외 은행 예금 등이다.\"},\n",
    "        {\"contexts\": \"제17조(배당소득) 배당소득은 국내외 법인으로부터 받는 배당금 및 배분금, 기타 법에서 정한 소득을 포함한다.\"}\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = 'tax-markdown-index'\n",
    "\n",
    "database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)\n",
    "retriever = database.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG bot\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "        # 위에서 선언한 retriever를 할용해서 Retrieval 실행\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        # LangSmith 문법\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        # `retrieve_docs()` 를 통해 가져온 문서들을 system prompt로 전달\n",
    "        # 3.3에서 했던 방식과 유사함\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"당신은 한국의 소득세 전문가입니다.\"\n",
    "                    \"아래 소득세법을 참고해서 사용자의 질문에 답변해주세요.\\n\\n\"\n",
    "                    f\"## 소득세법\\n\\n{docs}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Evaluators 를 활용해서 `answer`와 `context`를 평가할 예정\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"답변만 평가할 때 사용\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Context를 활용해서 hallucination을 평가할 때 사용\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\client.py:5301: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "# 답변의 정확도를 측정하기위해 사용되는 프롬프트\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    RAG 답변 성능을 측정하기 위한 evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # `example`이 데이터를 생성할 때 입력한 `Question-Answer` pair. `run`은 `RagBot`을 활용해서 생성한 LLM의 답변\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "# 답변이 사용자의 질문에 얼마나 도움되는지 판단하는 프롬프트\n",
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    답변이 사용자의 질문에 얼마나 도움되는지 판단하는 Evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터셋의 답변과 비교하지 않고, 데이터셋의 질문에 대한 LLM의 답변의 가치를 평가함\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "# hallucination 판단을 위한 프롬프트\n",
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    hallucination 판단을 위한 Evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터셋에 있는 질문과, LLM이 답변을 생성할 때 사용한 context를 활용\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # LLM의 답변\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'inflearn-evaluator-lecture-hallucination-d78ae7e2' at:\n",
      "https://smith.langchain.com/o/79663176-5928-5984-985d-f3b2690bc6ab/datasets/649f8820-1b92-402a-9d3c-1d79780b0bfd/compare?selectedSessions=5cd8dec2-6966-4165-9e56-7229cd7c11ea\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 3cb4b4a5-b96a-4fd2-a222-25dbdae83cbd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29868, Requested 521. Please try again in 778ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29868, Requested 521. Please try again in 778ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run fcb81652-c2f6-48ba-be04-14528bc97abc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29728, Requested 416. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 22, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29728, Requested 416. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 8fdbc942-0f2c-4504-b921-5ab6e639becf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29611, Requested 525. Please try again in 272ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29611, Requested 525. Please try again in 272ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 6c0ed910-6060-4419-acd8-579cb31415ab: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29780, Requested 605. Please try again in 770ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29780, Requested 605. Please try again in 770ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29583, Requested 3293. Please try again in 5.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run a1cf28c2-384e-4c85-aa2d-12d804fc2c00: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run a1cf28c2-384e-4c85-aa2d-12d804fc2c00: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run a1cf28c2-384e-4c85-aa2d-12d804fc2c00: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29538, Requested 3460. Please try again in 5.996s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "1it [00:09,  9.97s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run ec0914e0-227e-4c57-b50c-1722c18d97b6: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run ec0914e0-227e-4c57-b50c-1722c18d97b6: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run ec0914e0-227e-4c57-b50c-1722c18d97b6: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 6c0ed910-6060-4419-acd8-579cb31415ab: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29814, Requested 511. Please try again in 650ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 22, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29814, Requested 511. Please try again in 650ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e73eac46-9c1a-4191-a0c0-b14877cce680: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29716, Requested 746. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29716, Requested 746. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run fb969681-4a2e-412f-b7b9-964a1ffa382e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29681, Requested 779. Please try again in 920ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29681, Requested 779. Please try again in 920ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run c935b032-2325-4cb1-98f7-3e1f2e439af9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29584, Requested 868. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29584, Requested 868. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run fb969681-4a2e-412f-b7b9-964a1ffa382e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29334, Requested 685. Please try again in 38ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 22, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29334, Requested 685. Please try again in 38ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run c935b032-2325-4cb1-98f7-3e1f2e439af9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29239, Requested 766. Please try again in 10ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 22, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29239, Requested 766. Please try again in 10ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 28077, Requested 3241. Please try again in 2.636s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run aa75ba88-c0b6-4088-817e-d5e80f2ae94c: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run aa75ba88-c0b6-4088-817e-d5e80f2ae94c: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run aa75ba88-c0b6-4088-817e-d5e80f2ae94c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 27161, Requested 3592. Please try again in 1.506s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "3it [00:17,  5.31s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e9e60984-687a-4c08-b215-eb5eb8e893fe: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run e9e60984-687a-4c08-b215-eb5eb8e893fe: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run e9e60984-687a-4c08-b215-eb5eb8e893fe: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 26770, Requested 3244. Please try again in 28ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "4it [00:18,  3.74s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run fd639a1a-0a3a-431d-912a-55023b338f29: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 26770, Requested 3246. Please try again in 32ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run fd639a1a-0a3a-431d-912a-55023b338f29: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 793ae455-7acc-4274-bbf7-b8f30a4af12d: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run fd639a1a-0a3a-431d-912a-55023b338f29: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 793ae455-7acc-4274-bbf7-b8f30a4af12d: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 793ae455-7acc-4274-bbf7-b8f30a4af12d: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29876, Requested 3361. Please try again in 6.474s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29879, Requested 3371. Please try again in 6.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "6it [00:18,  2.03s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 042c747b-f42e-46cc-92c6-af270a0cdb7f: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 042c747b-f42e-46cc-92c6-af270a0cdb7f: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 042c747b-f42e-46cc-92c6-af270a0cdb7f: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e7609717-1f6a-4a7f-9a47-309c85892852: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run e7609717-1f6a-4a7f-9a47-309c85892852: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run e7609717-1f6a-4a7f-9a47-309c85892852: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29852, Requested 3369. Please try again in 6.442s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 90d6d542-c40a-4f5f-9ef1-338f5253d1b4: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 90d6d542-c40a-4f5f-9ef1-338f5253d1b4: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 90d6d542-c40a-4f5f-9ef1-338f5253d1b4: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29850, Requested 3376. Please try again in 6.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 25368206-e0dc-49d8-a533-110aa63d59be: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3649058899.py\", line 16, in answer_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 25368206-e0dc-49d8-a533-110aa63d59be: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\1176638789.py\", line 12, in answer_helpfulness_evaluator\n",
      "    prediction = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 25368206-e0dc-49d8-a533-110aa63d59be: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 12, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 8ffa262d-3e6e-4adf-806c-a98642e6d25a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29833, Requested 3400. Please try again in 6.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29833, Requested 3400. Please try again in 6.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 3cb4b4a5-b96a-4fd2-a222-25dbdae83cbd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29682, Requested 3556. Please try again in 6.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29682, Requested 3556. Please try again in 6.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run fcb81652-c2f6-48ba-be04-14528bc97abc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29590, Requested 3652. Please try again in 6.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29590, Requested 3652. Please try again in 6.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run fdf675be-cb7a-436b-a91e-05e5d30adbac: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29578, Requested 3664. Please try again in 6.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29578, Requested 3664. Please try again in 6.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 6c0ed910-6060-4419-acd8-579cb31415ab: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29340, Requested 3886. Please try again in 6.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29340, Requested 3886. Please try again in 6.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run fb969681-4a2e-412f-b7b9-964a1ffa382e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29709, Requested 3854. Please try again in 7.126s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29709, Requested 3854. Please try again in 7.126s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run c935b032-2325-4cb1-98f7-3e1f2e439af9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29477, Requested 4067. Please try again in 7.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29477, Requested 4067. Please try again in 7.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run e73eac46-9c1a-4191-a0c0-b14877cce680: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29450, Requested 4104. Please try again in 7.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ha018\\AppData\\Local\\Temp\\ipykernel_24200\\3922730770.py\", line 25, in answer_hallucination_evaluator\n",
      "    score = answer_grader.invoke({\"documents\": contexts,\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3013, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5313, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 855, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 670, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\ha018\\Documents\\GitHub\\inflearn_lecture\\inflearn_llm_application\\inflearn-llm-application\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-AxZ3yhVgWLNqlBI0s0YBkimb on tokens per min (TPM): Limit 30000, Used 29450, Requested 4104. Please try again in 7.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "20it [00:40,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"income_tax_dataset\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context, # 어떤 함수를 활용해서 LLM 답변을 확인할지 지정, hallucination 판단 여부에 따라 `with_context` 사용\n",
    "    data=dataset_name, # Evaluation에 사용될 dataset의 이름\n",
    "    evaluators=[answer_evaluator, answer_helpfulness_evaluator, answer_hallucination_evaluator], # 실행할 Evaluator의 종류\n",
    "    experiment_prefix=\"inflearn-evaluator-lecture-hallucination\",\n",
    "    metadata={\"version\": \"income tax v1, gpt-4o\"}, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn-llm-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
